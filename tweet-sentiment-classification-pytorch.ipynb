{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9444bc7f",
   "metadata": {
    "papermill": {
     "duration": 0.008537,
     "end_time": "2023-01-30T14:25:16.063421",
     "exception": false,
     "start_time": "2023-01-30T14:25:16.054884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will explore Tweet Sentiment Classification based on the tweet sentiment challenge from Kaggle. The datasets are provided in the link below.\n",
    "\n",
    "(https://www.kaggle.com/c/tweet-sentiment-extraction/overview) \n",
    "<br>\n",
    "#### Here is the description from the challenge:\n",
    "\n",
    "\"My ridiculous dog is amazing.\" [sentiment: positive]\n",
    "\n",
    "With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n",
    "\n",
    "Help build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?\n",
    "\n",
    "In this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.\n",
    "\n",
    "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n",
    "\n",
    "Files\n",
    "train.csv - the training set \\\n",
    "test.csv - the test set \\\n",
    "\n",
    "Columns\\\n",
    "textID - unique ID for each piece of text\\\n",
    "text - the text of the tweet\\\n",
    "sentiment - the general sentiment of the tweet\\\n",
    "selected_text - [train only] the text that supports the tweet's sentiment\n",
    "\n",
    "<br>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72fef9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:16.081561Z",
     "iopub.status.busy": "2023-01-30T14:25:16.080591Z",
     "iopub.status.idle": "2023-01-30T14:25:17.158002Z",
     "shell.execute_reply": "2023-01-30T14:25:17.156467Z"
    },
    "papermill": {
     "duration": 1.090255,
     "end_time": "2023-01-30T14:25:17.161076",
     "exception": false,
     "start_time": "2023-01-30T14:25:16.070821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import shutil\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9210f84",
   "metadata": {
    "papermill": {
     "duration": 0.007329,
     "end_time": "2023-01-30T14:25:17.176637",
     "exception": false,
     "start_time": "2023-01-30T14:25:17.169308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****\n",
    "Let us define methods to pre-process the review data\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d7e2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:17.196381Z",
     "iopub.status.busy": "2023-01-30T14:25:17.195541Z",
     "iopub.status.idle": "2023-01-30T14:25:17.207449Z",
     "shell.execute_reply": "2023-01-30T14:25:17.205968Z"
    },
    "papermill": {
     "duration": 0.025648,
     "end_time": "2023-01-30T14:25:17.210427",
     "exception": false,
     "start_time": "2023-01-30T14:25:17.184779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_url(string): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', string)\n",
    " # converting return value from list to string\n",
    "\n",
    "def clean_text(text): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))]) \n",
    "    \n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eaf67e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:17.226833Z",
     "iopub.status.busy": "2023-01-30T14:25:17.226469Z",
     "iopub.status.idle": "2023-01-30T14:25:17.231441Z",
     "shell.execute_reply": "2023-01-30T14:25:17.230363Z"
    },
    "papermill": {
     "duration": 0.015515,
     "end_time": "2023-01-30T14:25:17.233439",
     "exception": false,
     "start_time": "2023-01-30T14:25:17.217924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to label tweet as positive, negative or neutral\n",
    "def get_sentiment(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 2\n",
    "    elif sentiment == 'negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f70b0c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:17.249395Z",
     "iopub.status.busy": "2023-01-30T14:25:17.248991Z",
     "iopub.status.idle": "2023-01-30T14:25:17.424813Z",
     "shell.execute_reply": "2023-01-30T14:25:17.423797Z"
    },
    "papermill": {
     "duration": 0.187296,
     "end_time": "2023-01-30T14:25:17.427916",
     "exception": false,
     "start_time": "2023-01-30T14:25:17.240620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data= pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\n",
    "train_data.head()\n",
    "# in this exercise, we will only consider 'text' and ignore 'selected_text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eba5495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:17.445789Z",
     "iopub.status.busy": "2023-01-30T14:25:17.445426Z",
     "iopub.status.idle": "2023-01-30T14:25:17.484486Z",
     "shell.execute_reply": "2023-01-30T14:25:17.483499Z"
    },
    "papermill": {
     "duration": 0.050694,
     "end_time": "2023-01-30T14:25:17.486913",
     "exception": false,
     "start_time": "2023-01-30T14:25:17.436219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data= pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62079755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:17.505141Z",
     "iopub.status.busy": "2023-01-30T14:25:17.504662Z",
     "iopub.status.idle": "2023-01-30T14:25:18.234169Z",
     "shell.execute_reply": "2023-01-30T14:25:18.232978Z"
    },
    "papermill": {
     "duration": 0.741702,
     "end_time": "2023-01-30T14:25:18.237423",
     "exception": false,
     "start_time": "2023-01-30T14:25:17.495721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Train data--------\n",
      "neutral     10704\n",
      "positive     8375\n",
      "negative     7673\n",
      "Name: sentiment, dtype: int64\n",
      "26752\n",
      "-------------------------\n",
      "-------Test data--------\n",
      "neutral     1376\n",
      "positive    1075\n",
      "negative     983\n",
      "Name: sentiment, dtype: int64\n",
      "3434\n",
      "-------------------------\n",
      "Train Max Sentence Length :33\n",
      "Test Max Sentence Length :32\n"
     ]
    }
   ],
   "source": [
    "train_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "train_data['Num_words_text'] = train_data['text'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "#keep only rows with more than 2 words\n",
    "train_data = train_data[train_data['Num_words_text'] >2]\n",
    "print('-------Train data--------')\n",
    "print(train_data['sentiment'].value_counts())\n",
    "print(len(train_data))\n",
    "print('-------------------------')\n",
    "max_train_sentence_length  = train_data['Num_words_text'].max()\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(remove_emoji)\n",
    "train_data['text'] = train_data['text'].apply(remove_url)\n",
    "train_data['text'] = train_data['text'].apply(clean_text)\n",
    "\n",
    "train_data['label'] = train_data['sentiment'].apply(get_sentiment)\n",
    "\n",
    "test_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "test_data['Num_words_text'] = test_data['text'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "max_test_sentence_length  = test_data['Num_words_text'].max()\n",
    "\n",
    "#keep only rows with more than 2 words\n",
    "test_data = test_data[test_data['Num_words_text'] >2]\n",
    "\n",
    "print('-------Test data--------')\n",
    "print(test_data['sentiment'].value_counts())\n",
    "print(len(test_data))\n",
    "print('-------------------------')\n",
    "\n",
    "test_data['text'] = test_data['text'].apply(remove_emoji)\n",
    "test_data['text'] = test_data['text'].apply(remove_url)\n",
    "test_data['text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "test_data['label'] = test_data['sentiment'].apply(get_sentiment)\n",
    "\n",
    "print('Train Max Sentence Length :'+str(max_train_sentence_length))\n",
    "print('Test Max Sentence Length :'+str(max_test_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f28ae1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:18.255447Z",
     "iopub.status.busy": "2023-01-30T14:25:18.255072Z",
     "iopub.status.idle": "2023-01-30T14:25:18.267851Z",
     "shell.execute_reply": "2023-01-30T14:25:18.266811Z"
    },
    "papermill": {
     "duration": 0.02408,
     "end_time": "2023-01-30T14:25:18.270118",
     "exception": false,
     "start_time": "2023-01-30T14:25:18.246038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Num_words_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>have responded were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad will miss you here san diego</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>boss bullying</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview leave alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>sons why couldnt they put them the releases al...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                          have responded were going   \n",
       "1  549e992a42              sooo sad will miss you here san diego   \n",
       "2  088c60f138                                      boss bullying   \n",
       "3  9642c003ef                         what interview leave alone   \n",
       "4  358bd9e861  sons why couldnt they put them the releases al...   \n",
       "\n",
       "                         selected_text sentiment  Num_words_text  label  \n",
       "0  I`d have responded, if I were going   neutral               7      0  \n",
       "1                             Sooo SAD  negative              10      1  \n",
       "2                          bullying me  negative               5      1  \n",
       "3                       leave me alone  negative               5      1  \n",
       "4                        Sons of ****,  negative              14      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06b0c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:18.288315Z",
     "iopub.status.busy": "2023-01-30T14:25:18.287927Z",
     "iopub.status.idle": "2023-01-30T14:25:18.300068Z",
     "shell.execute_reply": "2023-01-30T14:25:18.298674Z"
    },
    "papermill": {
     "duration": 0.02386,
     "end_time": "2023-01-30T14:25:18.302336",
     "exception": false,
     "start_time": "2023-01-30T14:25:18.278476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Num_words_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>last session the day</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>shanghai also really exciting precisely skyscr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>recession hit veronique branquinho she has qui...</td>\n",
       "      <td>negative</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>like</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>726e501993</td>\n",
       "      <td>thats great weee visitors</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment  \\\n",
       "0  f87dea47db                               last session the day   neutral   \n",
       "1  96d74cb729  shanghai also really exciting precisely skyscr...  positive   \n",
       "2  eee518ae67  recession hit veronique branquinho she has qui...  negative   \n",
       "4  33987a8ee5                                               like  positive   \n",
       "5  726e501993                          thats great weee visitors  positive   \n",
       "\n",
       "   Num_words_text  label  \n",
       "0               6      0  \n",
       "1              15      2  \n",
       "2              13      1  \n",
       "4               5      2  \n",
       "5               4      2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809dfa5",
   "metadata": {
    "papermill": {
     "duration": 0.008135,
     "end_time": "2023-01-30T14:25:18.318952",
     "exception": false,
     "start_time": "2023-01-30T14:25:18.310817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "Let us create our train,valid and test datasets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5280d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:18.337545Z",
     "iopub.status.busy": "2023-01-30T14:25:18.337108Z",
     "iopub.status.idle": "2023-01-30T14:25:18.380664Z",
     "shell.execute_reply": "2023-01-30T14:25:18.379418Z"
    },
    "papermill": {
     "duration": 0.055932,
     "end_time": "2023-01-30T14:25:18.383216",
     "exception": false,
     "start_time": "2023-01-30T14:25:18.327284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:21401\n",
      "Class distributionCounter({0: 8563, 2: 6700, 1: 6138})\n",
      "Valid data len:5351\n",
      "Class distributionCounter({0: 2141, 2: 1675, 1: 1535})\n",
      "Test data len:3434\n",
      "Class distributionCounter({0: 1376, 2: 1075, 1: 983})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid= train_test_split(train_data['text'].tolist(),\\\n",
    "                                                      train_data['label'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = train_data['label'].tolist(),\\\n",
    "                                                      random_state=0)\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(Y_train)))\n",
    "\n",
    "\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(Y_valid)))\n",
    "\n",
    "print('Test data len:'+str(len(test_data['text'].tolist())))\n",
    "print('Class distribution'+ str(Counter(test_data['label'].tolist())))\n",
    "\n",
    "\n",
    "train_dat =list(zip(Y_train,X_train))\n",
    "valid_dat =list(zip(Y_valid,X_valid))\n",
    "test_dat=list(zip(test_data['label'].tolist(),test_data['text'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5766d3ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:18.401315Z",
     "iopub.status.busy": "2023-01-30T14:25:18.400933Z",
     "iopub.status.idle": "2023-01-30T14:25:19.961753Z",
     "shell.execute_reply": "2023-01-30T14:25:19.960798Z"
    },
    "papermill": {
     "duration": 1.572619,
     "end_time": "2023-01-30T14:25:19.964239",
     "exception": false,
     "start_time": "2023-01-30T14:25:18.391620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfc08f",
   "metadata": {
    "papermill": {
     "duration": 0.008153,
     "end_time": "2023-01-30T14:25:19.981105",
     "exception": false,
     "start_time": "2023-01-30T14:25:19.972952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "Let us create our vocabulary on train data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64ded57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:19.999526Z",
     "iopub.status.busy": "2023-01-30T14:25:19.998988Z",
     "iopub.status.idle": "2023-01-30T14:25:20.631326Z",
     "shell.execute_reply": "2023-01-30T14:25:20.630107Z"
    },
    "papermill": {
     "duration": 0.644628,
     "end_time": "2023-01-30T14:25:20.634001",
     "exception": false,
     "start_time": "2023-01-30T14:25:19.989373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = train_dat\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87483b1",
   "metadata": {
    "papermill": {
     "duration": 0.008145,
     "end_time": "2023-01-30T14:25:20.650699",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.642554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us define our text and label preprocessing pipleines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0165c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:20.668968Z",
     "iopub.status.busy": "2023-01-30T14:25:20.668214Z",
     "iopub.status.idle": "2023-01-30T14:25:20.672873Z",
     "shell.execute_reply": "2023-01-30T14:25:20.672142Z"
    },
    "papermill": {
     "duration": 0.015989,
     "end_time": "2023-01-30T14:25:20.674860",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.658871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e1c923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:20.693234Z",
     "iopub.status.busy": "2023-01-30T14:25:20.692340Z",
     "iopub.status.idle": "2023-01-30T14:25:20.698127Z",
     "shell.execute_reply": "2023-01-30T14:25:20.697435Z"
    },
    "papermill": {
     "duration": 0.016861,
     "end_time": "2023-01-30T14:25:20.700062",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.683201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 0, 1, 0, 12881]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e77356b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:20.718822Z",
     "iopub.status.busy": "2023-01-30T14:25:20.718160Z",
     "iopub.status.idle": "2023-01-30T14:25:20.723847Z",
     "shell.execute_reply": "2023-01-30T14:25:20.722846Z"
    },
    "papermill": {
     "duration": 0.017775,
     "end_time": "2023-01-30T14:25:20.726289",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.708514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecf131",
   "metadata": {
    "papermill": {
     "duration": 0.00814,
     "end_time": "2023-01-30T14:25:20.742725",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.734585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us define our batch collation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f12389f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:20.761526Z",
     "iopub.status.busy": "2023-01-30T14:25:20.760484Z",
     "iopub.status.idle": "2023-01-30T14:25:20.768520Z",
     "shell.execute_reply": "2023-01-30T14:25:20.767740Z"
    },
    "papermill": {
     "duration": 0.019497,
     "end_time": "2023-01-30T14:25:20.770522",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.751025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "#train_iter =train_dat\n",
    "#dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368042c",
   "metadata": {
    "papermill": {
     "duration": 0.008212,
     "end_time": "2023-01-30T14:25:20.787168",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.778956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us define our text classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f920a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:20.805863Z",
     "iopub.status.busy": "2023-01-30T14:25:20.805303Z",
     "iopub.status.idle": "2023-01-30T14:25:20.815225Z",
     "shell.execute_reply": "2023-01-30T14:25:20.814459Z"
    },
    "papermill": {
     "duration": 0.021828,
     "end_time": "2023-01-30T14:25:20.817281",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.795453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,64)\n",
    "        self.fc2 = nn.Linear(64,16)\n",
    "        self.fc3 = nn.Linear(16, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d08271",
   "metadata": {
    "papermill": {
     "duration": 0.008137,
     "end_time": "2023-01-30T14:25:20.834065",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.825928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us create an object of our text classification class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc7388c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:20.852715Z",
     "iopub.status.busy": "2023-01-30T14:25:20.852084Z",
     "iopub.status.idle": "2023-01-30T14:25:20.918983Z",
     "shell.execute_reply": "2023-01-30T14:25:20.917762Z"
    },
    "papermill": {
     "duration": 0.079118,
     "end_time": "2023-01-30T14:25:20.921486",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.842368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "23296\n"
     ]
    }
   ],
   "source": [
    "train_iter1 = train_dat\n",
    "num_class = len(set([label for (label, text) in train_iter1]))\n",
    "print(num_class)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "emsize = 128\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f971f539",
   "metadata": {
    "papermill": {
     "duration": 0.008237,
     "end_time": "2023-01-30T14:25:20.938448",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.930211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us define our train and evaluate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d93fcb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:20.957758Z",
     "iopub.status.busy": "2023-01-30T14:25:20.956729Z",
     "iopub.status.idle": "2023-01-30T14:25:20.967079Z",
     "shell.execute_reply": "2023-01-30T14:25:20.966299Z"
    },
    "papermill": {
     "duration": 0.022014,
     "end_time": "2023-01-30T14:25:20.969112",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.947098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16107522",
   "metadata": {
    "papermill": {
     "duration": 0.008234,
     "end_time": "2023-01-30T14:25:20.985860",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.977626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us create dataloaders for text,train and validation data iterators and then train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e37de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:21.004864Z",
     "iopub.status.busy": "2023-01-30T14:25:21.004014Z",
     "iopub.status.idle": "2023-01-30T14:25:50.235210Z",
     "shell.execute_reply": "2023-01-30T14:25:50.233963Z"
    },
    "papermill": {
     "duration": 29.243145,
     "end_time": "2023-01-30T14:25:50.237440",
     "exception": false,
     "start_time": "2023-01-30T14:25:20.994295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1338 batches | accuracy    0.398\n",
      "| epoch   1 |  1000/ 1338 batches | accuracy    0.503\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.93s | valid accuracy    0.492 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1338 batches | accuracy    0.593\n",
      "| epoch   2 |  1000/ 1338 batches | accuracy    0.601\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.90s | valid accuracy    0.611 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1338 batches | accuracy    0.648\n",
      "| epoch   3 |  1000/ 1338 batches | accuracy    0.651\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.96s | valid accuracy    0.648 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1338 batches | accuracy    0.685\n",
      "| epoch   4 |  1000/ 1338 batches | accuracy    0.684\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.93s | valid accuracy    0.654 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1338 batches | accuracy    0.715\n",
      "| epoch   5 |  1000/ 1338 batches | accuracy    0.707\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.91s | valid accuracy    0.644 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1338 batches | accuracy    0.782\n",
      "| epoch   6 |  1000/ 1338 batches | accuracy    0.784\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  3.02s | valid accuracy    0.677 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1338 batches | accuracy    0.804\n",
      "| epoch   7 |  1000/ 1338 batches | accuracy    0.801\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.92s | valid accuracy    0.678 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1338 batches | accuracy    0.818\n",
      "| epoch   8 |  1000/ 1338 batches | accuracy    0.810\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.88s | valid accuracy    0.682 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1338 batches | accuracy    0.831\n",
      "| epoch   9 |  1000/ 1338 batches | accuracy    0.835\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.88s | valid accuracy    0.681 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1338 batches | accuracy    0.859\n",
      "| epoch  10 |  1000/ 1338 batches | accuracy    0.853\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.90s | valid accuracy    0.683 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR =10  # learning rate\n",
    "BATCH_SIZE = 16 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter2 = train_dat\n",
    "test_iter2 =test_dat \n",
    "valid_iter2= valid_dat\n",
    "\n",
    "train_dataloader = DataLoader(train_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_iter2, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a77d4f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:50.260864Z",
     "iopub.status.busy": "2023-01-30T14:25:50.259865Z",
     "iopub.status.idle": "2023-01-30T14:25:50.422164Z",
     "shell.execute_reply": "2023-01-30T14:25:50.420937Z"
    },
    "papermill": {
     "duration": 0.177216,
     "end_time": "2023-01-30T14:25:50.425478",
     "exception": false,
     "start_time": "2023-01-30T14:25:50.248262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.681\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab06e3",
   "metadata": {
    "papermill": {
     "duration": 0.010194,
     "end_time": "2023-01-30T14:25:50.446938",
     "exception": false,
     "start_time": "2023-01-30T14:25:50.436744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Test model on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a5d51be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:25:50.468797Z",
     "iopub.status.busy": "2023-01-30T14:25:50.468420Z",
     "iopub.status.idle": "2023-01-30T14:25:50.478929Z",
     "shell.execute_reply": "2023-01-30T14:25:50.477552Z"
    },
    "papermill": {
     "duration": 0.024222,
     "end_time": "2023-01-30T14:25:50.481341",
     "exception": false,
     "start_time": "2023-01-30T14:25:50.457119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Neutral tweet\n"
     ]
    }
   ],
   "source": [
    "sentiment_label = {2:\"Positive\",\n",
    "                   1: \"Negative\",\n",
    "                   0: \"Neutral\"\n",
    "                  }\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "ex_text_str = \"soooooo wish i could, but im in school and myspace is completely blocked\"\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s tweet\" %sentiment_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfeff4e",
   "metadata": {
    "papermill": {
     "duration": 0.010046,
     "end_time": "2023-01-30T14:25:50.501794",
     "exception": false,
     "start_time": "2023-01-30T14:25:50.491748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874316fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-30T14:20:23.522516Z",
     "iopub.status.busy": "2023-01-30T14:20:23.521911Z",
     "iopub.status.idle": "2023-01-30T14:20:23.529659Z",
     "shell.execute_reply": "2023-01-30T14:20:23.528355Z",
     "shell.execute_reply.started": "2023-01-30T14:20:23.522469Z"
    },
    "papermill": {
     "duration": 0.009888,
     "end_time": "2023-01-30T14:25:50.522056",
     "exception": false,
     "start_time": "2023-01-30T14:25:50.512168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A lot of functions were provided in the tutorials on pytorch website\\\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html?highlight=text%20classification\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.36384,
   "end_time": "2023-01-30T14:25:51.353785",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-30T14:25:07.989945",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
